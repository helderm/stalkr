\section{Method}
\label{sec:method}

\input{method_twitter}

\subsection{The Neo4j database}

Neo4j is a graph database that removes the need to explicitly define a schema
for the relationships between entities. It has efficient techniques for storing
graphs, making it suitable for storing large amounts of data with many
relationships. Neo4j allow us to represent the full structure of the Twitter
database as a graph, where each different entity is a \emph{node} and the
relationship between the nodes an \emph{edge}. Figure \ref{fig:schema} details
the nodes and the edges of our recommender engine.


% In order to store all the information fetched from Twitter, a suitable database
% had to be chosen beforehand. We wanted a solution where the relationship between
% the different entities could be easily seen and queried, and that could
% effortlessly store the large amount of data that we planned to fetch. Our
% approach utilizes a \emph{graph database} as a solution to our storing problem,
% more specifically a software called \emph{Neo4j}\cite{neo4j}. This software
% allow us to represent the full structure of the Twitter database as a graph,
% where each different entity is a \emph{node} and the relationship between the
% nodes an \emph{edge}. Figure \ref{fig:schema} details the nodes and the edges of
% our recommender engine.

The base node of the graph is the \emph{Tweet}, which is a free-text short
message sent through Twitter. Every tweet is posted by a \emph{User}, which is
represented by the \emph{post} relationship. A tweet may \emph{mention} another
user with the \emph{@} special character, and it may also \emph{tag} a topic
with \emph{\#}, represented by the node \emph{Hash}. To this basic schema
derived directly from Twitter, we added the node \emph{Word} which are all the
parsed words in the free text of the Tweet. This node is linked to the rest of
the graph through the \emph{contains} and \emph{discusses} relationships, with a
property specifying the amount of times this relationship happens for every
tweet and user. This new entity allows us to easily represent the user as a
\emph{bag-of-words} document of every word that the user discusses in all of the
user's tweets, thus allowing us to map the recommendation problem to the
standard techniques used on information retrieval.

\begin{figure}[t]
\centering
\includegraphics[width=3.0in,natwidth=440,natheight=409]{images/Schema.png}
\caption{Graph schema used for the Twitter data.}
\label{fig:schema}
\end{figure}

\subsection{Tweet text processing}

The topics of the tweets are extracted by parsing their freetext and
finding nouns and adjectives. That was an empiric decision. This is done 
using string processing alongside the Natural Language Toolkit (NLTK) 
\cite{bird2006nltk} which provides interfaces in Python for classification, 
tokenization and stemming.

\subsubsection{Cleaning tweets}

A tweet can contain hyperlinks, hashtags, mentions and other symbols. These are
removed in order to properly parse the text of the tweet. Specifically, words
starting with \verb|#, @, &, http| are ignored. A few other words that
commonly occur in a tweet were also ignored as they would not contribute to the
cause. These are \textit{don't, i'll, retweet} and \textit{rt}.

\subsubsection{Extracting topics}

First, letters are lowercased and the text is tokenized, then words shorter than
three characters are removed, after which ignored symbols are removed. Finally,
we use NLTK to Part of Speech-tag \cite{pos} each term so that we can pick only
\texttt{NN}s (nouns) and \texttt{JJ}s (adjectives) and stem these terms, which are
returned as a list.

% \subsection{Parsing tweets and extracting topics}

% The goal of the project is to recommend users given topics. In order to
% recommend a user, the user needs to be associated with the topics the user talks
% about. Therefore the users tweets are parsed and the topics of the tweets are
% extracted.  The topics are extracted by parsing the freetext of the tweets and
% extracting the nouns and adjectives.  The choice of extracting nouns and
% adjectives was an empiric decision made by the group.

% Extracting topics from tweets is done using the Natural Language Toolkit (NLTK)
% \cite{bird2006nltk} which provides interfaces in Python for things like
% classification, tokenization and stemming.

% \subsubsection{Cleaning tweets}

% A tweet can contain hyperlinks, hashtags, mentions and other symbols. These are
% removed in order to properly parse the text of the tweet. Specifically, words
% starting with \textit{\#, @, \& or http} are ignored. A few other words that
% commonly occur in a tweet were also ignored as they would not contribute to the
% cause. These are \textit{don't, i'll, retweet and rt}.

% \subsubsection{Extracting nouns}

% The nouns (topics) are extracted by performing the following actions, provided
% by NLTK:

% \begin{enumerate}
%     \item Lowercase all letters and tokenize the text into separate tokens
%     \item Remove words that are shorter than three characters (This was also a
%           decision made by the group)
%     \item For each word, remove ignored symbols and words starting with a
% 	    ignored symbol
%     \item Part of Speech-tag \cite{pos} the words
%     \item Pick the words that are tagged as \texttt{NN} (noun) or \texttt{JJ}
%         (adjective)
%     \item Stem the words and return the result which is a list of words
% \end{enumerate}

\input{method_pr}

\input{method_tfidf}

\input{method_finalscore}

\subsection{Word2vec - Generating Synonyms}
Word embeddings, also referred to as distributed representations of words, is a method for generating representation of words which capture their meaning and relationships to other words. This is achieved by having each word represented by a real vector, and therefore maps each word into a multi-dimensional continuous space. In accordance to the distributional hypothesis, words which are used in similar contexts, and therefore have similar vector representations, should be mapped close to each other in the vector space. One of the most basic and widely used models is the Continuous Bag of Word (CBOW) model as described in \cite{wordRep}, which takes a context of words as input and computes the most likely target word in that context. The context words are encoded as 1-of-V vectors, where V is the size of the vocabulary, and these vectors are then averaged into one final context vector to be multiplied by the trained weight matrix of the network \cite{wordRep}.

The model used in this paper is trained using the word2vec toolkit by Google Inc \cite{word2vec}. The dataset used is the text8 file, a 97 567 kB file available from \cite{dataset}. For the training, the CBOW model was run with a word vector size of 200. The window was set to 8, negative sampling was set to 25 and 20 thread were run. Sample was set to 1e-4, binary to 1 and iterations to 15. To extract the synonyms from the trained model, the cosine and the vocab methods were called on the loaded model. The input words and the output synonyms were also cleaned in the same way as the tweets in order to gain more consistency in relation to the database.
