\section{Method}
\label{sec:method}

\input{method_twitter}

\subsection{The Neo4j database}

In order to store all the information fetched from Twitter, a suitable database had to be chosen beforehand. We wanted a solution where the relationship between the different entities could be easily seen and queried, and that could effortlessly store the large amount of data that we planned to fetch. Our approach utilizes a \emph{graph database} as a solution to our storing problem, more specifically a software called \emph{Neo4j}\cite{neo4j}. This software allow us to faithfully represent the full structure of the Twitter database as a graph, where each different entity is a \emph{node} and the relationship between the nodes an \emph{edge}. Figure \ref{fig:schema} details the nodes and the edges of our recommender engine.

The base node of the graph is the \emph{Tweet}, which is a free text short message sent through Twitter. Every tweet is posted by an \emph{User}, which is represented by the \emph{post} relationship. A tweet may \emph{mention} another user with the \emph{@} special character, and it may also \emph{tag} a topic with \emph{\#}, represented by the node \emph{Hash}. To this basic schema derived directly from Twitter, we added the node \emph{Word} which are all the parsed words in the free text of the Tweet, properly tokenized and stemmed. This node is linked to the rest of the graph through the \emph{contains} and \emph{discusses} relationships, with a property specifying the amount of times this relationship happens for every tweet and user. This new entity allows us to easily represent the user as a \emph{bag-of-words} document of every word that he discusses about in all of his tweets, thus allowing us to map the recommendation problem to the standard techniques used on information retrieval. 

\begin{figure}[t]
\centering
\includegraphics[width=3.5in,natwidth=440,natheight=409]{images/Schema.png}
\caption{Graph schema used for the Twitter data.}
\label{fig:schema}
\end{figure}


\subsection{Parsing tweets and extracting topics}

The goal of the project is to recommend users given topics. In order to
recommend a user, the user needs to be associated with the topics the user talks
about. Therefore the users tweets are parsed and the topics of the tweets are
extracted.  The topics are extracted by parsing the freetext of the tweets and
extracting the nouns and adjectives.  The choice of extracting nouns and
adjectives was an empiric decision made by the group.

Extracting topics from tweets is done using the Natural Language Toolkit (NLTK)
\cite{bird2006nltk} which provides interfaces in Python for things like
classification, tokenization and stemming.

\subsubsection{Cleaning tweets}

A tweet can contain hyperlinks, hashtags, mentions and other symbols. These are
removed in order to properly parse the text of the tweet. Specifically, words
starting with \textit{\#, @, \& or http} are ignored. A few other words that
commonly occur in a tweet were also ignored as they would not contribute to the
cause. These are \textit{don't, i'll, retweet and rt}.

\subsubsection{Extracting nouns}

The nouns (topics) are extracted by performing the following actions, provided
by NLTK:

\begin{enumerate}
    \item Lowercase all letters and tokenize the text into separate tokens
    \item Remove words that are shorter than three characters (This was also a
          decision made by the group)
    \item For each word, remove ignored symbols and words starting with a
	    ignored symbol
    \item Part of Speech-tag \cite{pos} the words
    \item Pick the words that are tagged as \texttt{NN} (noun) or \texttt{JJ}
        (adjective)
    \item Stem the words and return the result which is a list of words
\end{enumerate}

\input{method_pr}

\input{method_tfidf}

\input{method_finalscore}


\subsection{Graphical user interface}
