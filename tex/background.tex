\section{Background}
\subsection{Word Embedding and The CBOW model}
Word embeddings, also referred to as distributed representations of words, is a more complex representation of words than the simpler way of letting words be represented by its index in the dictionary of words. Learning the meaning of words have demanded a better representation which can more effectively capture the the relationships of the words and its dimensionality. Word embedding achieves this by having each word represented by a real vector, and therefore maps each word into a multi-dimensional continuous space. In accordance to the distributional hypothesis, words which are used in similar contexts, and therefore have similar vector representations, should be mapped close to each other in the vector space, since they have similar meanings. One of the most basic and widely used models is the Continuous Bag of Word (CBOW) model as described in [4], which takes a context of words as input and computes the most likely target word in that context. The context words are encoded as 1-of-V vectors, where V is the size of the vocabulary, and these vectors are then averaged into one final context vector to be multiplied by the trained weight matrix of the network [4]. In other words, the order of the context words is lost and they are treated as a bag of words.
